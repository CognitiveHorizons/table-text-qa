{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "all_exs = json.load(open(\"/mnt/infonas/data/yashgupta/data/new_bs_processed_data/train_top_row_processed.json\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(all_exs[0].keys())\n",
    "print(all_exs[0])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['answer-text', 'context', 'title', 'question', 'question_id', 'is_impossible', 'answers'])\n",
      "{'answer-text': 'Ghurids', 'context': \"Name is Great Mosque of Herat . Province is Herat Province . City is Herat . Year is 1446 . Remarks is The mosque was the city 's first congregational mosque , built on the former site of two smaller Zoroastrian Fire temples destroyed by earthquake and fire . The Great Mosque of Herat ( Persian : مسجد جامع هرات , Masjid-i Jāmi-i Herāt ) or Jami Masjid of Herat , is a mosque in the city of Herat , in the Herat Province of north-western Afghanistan . It was built by the Ghurids , under the rule of the Sultan Ghiyath al-Din Muhammad Ghori , who laid its foundation in 1200 CE , and later extended by several rulers as Herat changed rulers down the centuries from the Timurids to the Safavids , Mughals and then the Uzbeks , all of whom supported the mosque . Though many of the glazed tiles have been replaced during subsequent periods , the Friday Mosque in Herat was given its present form during the closing years of the fifteenth century . Apart from numerous small neighborhood mosques for daily prayer , most communities in the Islamic world have a larger mosque , a congregational mosque for Friday services with a sermon . The Jama Masjid was not always the largest mosque in Herat ; a much larger complex the Mosque and Madressa of Gawhar Shad , also built by the Timurids , was located in the northern part of the city . However , those architectural monuments were dynamited by officers of the British Indian Army in 1885 , to prevent its use as a fortress if a Russian army tried to invade India . A fire temple in Zoroastrianism is the place of worship for Zoroastrians , often called dar-e mehr ( Persian ) or agiyari ( Gujarati ) . In the Zoroastrian religion , fire ( see atar ) , together with clean water ( see aban ) , are agents of ritual purity . Clean , white ash for the purification ceremonies [ is ] regarded as the basis of ritual life , which are essentially the rites proper to the tending of a domestic fire , for the temple [ fire ] is that of the hearth fire raised to a new solemnity . For , one who sacrifices unto fire with fuel in his hand . , is given happiness . As of 2019 [ update ] , there were 167 fire temples in the world , of which 45 were in Mumbai , 105 in the rest of India , and 17 in other countries . There is a religious custom in India of not allowing Zoroastrian women to enter the Fire Temple and the Tower of Silence if they marry a non-Zoroastrian person . This custom has been challenged before the Supreme Court of India after a Zoroastrian woman was denied entry into Zoroastrian institutions . Herāt ( /hɛˈrɑːt/ ; Persian/Pashto : هرات ) is the third-largest city of Afghanistan . It has a population of about 436,300 , and serves as the capital of Herat Province , situated in the fertile valley of the Hari River in the western part of the country . It is linked with Kandahar , Kabul , and Mazar-i-Sharif via Highway 1 or the ring road . It is further linked to the city of Mashhad in neighboring Iran through the border town of Islam Qala , and to Mary in Turkmenistan to the north through the border town of Torghundi . Herat dates back to Avestan times and was traditionally known for its wine . The city has a number of historic sites , including the Herat Citadel and the Musalla Complex . During the Middle Ages Herat became one of the important cities of Khorasan , as it was known as the Pearl of Khorasan . It has been governed by various Afghan rulers since the early 18th century . In 1717 , the city was invaded by the Hotaki forces until they were expelled by the Afsharids in 1729 . After Nader Shah 's death and Ahmad Shah Durrani 's rise to power in 1747 , Herat became part of Afghanistan . It witnessed some political disturbances and military invasions during the early half of the 19th century but the 1857 Treaty of Paris ended hostilities of the Anglo-Persian War . Herat lies on the ancient trade routes of the Middle East , Central and South Asia , and today is a regional hub in western Afghanistan . Herat ( Pashto/Dari : هرات ) is one of the thirty-four provinces of Afghanistan , located in the western part of the country . Together with Badghis , Farah , and Ghor provinces , it makes up the north-western region of Afghanistan . Its primary city and administrative capital is Herat City . The province of Herat is divided into about 17 districts and contains over 1,000 villages . It has a population of about 1,780,000 , making it the second most populated province in Afghanistan behind Kabul Province . The population is multi-ethnic but largely Persian-speaking . The province of Herat shares a border with Iran in the west and Turkmenistan in the north , making it an important trading region . The Trans-Afghanistan Pipeline ( TAPI ) is expected to pass through Herat from Turkmenistan to Pakistan and India in the south . The province has two airports , one is the Herat International Airport in the capital of Herat and the other is at the Shindand Air Base , which is one of the largest military bases in Afghanistan . The Salma Dam which is fed by the Hari River is also located in this province .\", 'title': 'List of mosques in Afghanistan 0', 'question': 'Who were the builders of the mosque in Herat with fire temples ?', 'question_id': '00009b9649d0dd0a_0', 'is_impossible': False, 'answers': [{'answer_start': 473, 'text': 'Ghurids'}]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re\n",
    "import copy\n",
    "cnt = 0\n",
    "sa_exs = []\n",
    "ma_exs = []\n",
    "for ex in all_exs:\n",
    "    ans = ex['answers'][0]['text']\n",
    "    if (ex['context']).count(ans) > 1:\n",
    "        ocs = [m.start() for m in re.finditer(re.escape(ans), ex['context'])]\n",
    "        # print(ocs)\n",
    "        # break\n",
    "        cnt += 1\n",
    "        for j, oc in enumerate(ocs):\n",
    "            exc = copy.deepcopy(ex)\n",
    "            exc['answers'][0]['answer_start'] = oc\n",
    "            # print(oc)\n",
    "            exc['question_id'] += str(j)\n",
    "            # print(exc)\n",
    "            ma_exs.append(exc)\n",
    "        # break\n",
    "    else:\n",
    "        sa_exs.append(ex)\n",
    "print(\"multi answer exs\", cnt) \n",
    "print(\"single answer exs\", len(sa_exs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "multi answer exs 19365\n",
      "single answer exs 36719\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "json.dump(sa_exs, open(\"/mnt/infonas/data/yashgupta/data/new_bs_processed_data/train_processed_top_row_no_group_sa.json\", \"w\"), indent=2)\n",
    "json.dump(ma_exs, open(\"/mnt/infonas/data/yashgupta/data/new_bs_processed_data/train_processed_top_row_no_group_ma.json\", \"w\"), indent=2)\n",
    "json.dump(ma_exs[:16], open(\"/mnt/infonas/data/yashgupta/data/new_bs_processed_data/train_processed_top_row_no_group_ma_toy.json\", \"w\"), indent=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "ma_scrs = json.load(open(\"/mnt/infonas/data/yashgupta/save/bert_large_top_no_group_sa/2021_09_04_19_52_05/checkpoint-epoch1/ma_full_loss_logits.json\"))\n",
    "print(len(ma_scrs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "343052\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "dct = {}\n",
    "for k, v in ma_scrs.items():\n",
    "    ks = k.split(\"_\")\n",
    "    tk = ks[0]\n",
    "    # print(ks[2])\n",
    "    if ks[2]!=\"loss\":\n",
    "        continue\n",
    "    if tk in dct.keys():\n",
    "        if dct[tk][0] > v:\n",
    "            dct[tk] = (v, k)\n",
    "    else:\n",
    "        dct[tk] = (v, k)\n",
    "print(len(dct.keys()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# dct"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "st = set([])\n",
    "for v in dct.values():\n",
    "    x = \"_\".join(v[1].split(\"_\")[:2])\n",
    "    st.add(x.rstrip('#'))\n",
    "print(len(st))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# print(st)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "ch_ma_ex  = []\n",
    "for ma_ex in ma_exs:\n",
    "    if ma_ex['question_id'] in st:\n",
    "        ch_ma_ex.append(ma_ex)\n",
    "print(len(ch_ma_ex))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19365\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "tot_exs = ch_ma_ex + sa_exs\n",
    "print(len(tot_exs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "56084\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "json.dump(tot_exs, open(\"/mnt/infonas/data/yashgupta/data/pl_processed_data/train_processed_top_row_no_group_total_loss.json\", \"w\"), indent=2)\n",
    "json.dump(tot_exs[:25], open(\"/mnt/infonas/data/yashgupta/data/processed_data/train_processed_top_row_no_group_total_loss_toy.json\", \"w\"), indent=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "cnt = 0\n",
    "for chx in ch_ma_ex:\n",
    "    if (chx['question_id'][-2:]!=\"00\"):\n",
    "        cnt += 1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7187\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import timeit\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from torch.utils.data import TensorDataset\n",
    "#TODO: For longformer\n",
    "#from transformers import LongformerModel, LongformerTokenizer\n",
    "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast,LongformerTokenizer, EvalPrediction, LongformerConfig\n",
    "from transformers import RobertaForQuestionAnswering, RobertaTokenizerFast,RobertaTokenizer,RobertaConfig\n",
    "\n",
    " \n",
    "from transformers import (WEIGHTS_NAME, AdamW, BertConfig, BertTokenizer, \n",
    "                        BertForQuestionAnswering, get_linear_schedule_with_warmup)\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    ")\n",
    "# from compute_predictions_logits import compute_predictions_logits\n",
    "import string\n",
    "from transformers.data.processors.utils import DataProcessor\n",
    "from utils.utils import readGZip\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# MODEL_CLASSES = {\"bert\": (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "#             ,\"longformer\":(LongformerConfig, LongformerForQuestionAnswering, LongformerTokenizer)\n",
    "#             ,\"roberta\":(RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer)}\n",
    "# config_class, model_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "# ckpt_pth = \"/mnt/infonas/data/yashgupta/save/bert_large_logits_top_all_sa/2021_08_30_14_12_37/checkpoint-epoch3/\"\n",
    "# tokenizer = tokenizer_class.from_pretrained(ckpt_pth)\n",
    "# model = model_class.from_pretrained(ckpt_pth)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 1.14 GiB already allocated; 5.62 MiB free; 1.14 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_342051/2500462504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/infonas/data/yashgupta/envs/ant_ttqa/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 1.14 GiB already allocated; 5.62 MiB free; 1.14 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ant_ttqa': venv)"
  },
  "interpreter": {
   "hash": "7ab22832012fa412923ae73333bc0dc2603956a3ce62a1d321457d390e693dc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}